{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Face Mask Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOk-psE_0D1"
      },
      "source": [
        "## Why do we need Mask Detection?\n",
        "In these uncertain times, human civilization needs to quickly adapt to the threat at hand. While wearing mask is not the ultimate solution, it still reduces the rate of transmission of the virus. For us, to create such safe ecosystem, we need techniques like Mask Detection to ensure that the rate of transmission is controlled in highly crowded public spaces and other areas where risk of transmission is high.\n",
        "\n",
        "In this notebook, following process is adopted:\n",
        "* Step1: Extract the face data for training.\n",
        "* Step2: Train the classifier to classify faces into mask or non-mask labels.\n",
        "* Step3: Detect Faces in testing data using SSD Face Detector.\n",
        "* Step4: By using the trained classifier, classify the detected faces.\n",
        "\n",
        "## Content\n",
        "1. [Importing Libraries and Directories](#section-one)<br>\n",
        "2. [What is SSD?](#section-two)<br>\n",
        "3. [Functions](#section-three) <br>\n",
        "4. [Data Preprocessing](#section-four)<br>\n",
        "5. [Model Architecture and Training Process](#section-five)<br>\n",
        "6. [Training and Validation Visualizations](#section-six)<br>\n",
        "7. [Model Testing](#section-seven)<br>\n",
        "8. [Conclusion](#section-eight)<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpK7YzvB_0D3"
      },
      "source": [
        "<a id=\"section-one\"></a>\n",
        "## Importing Libraries and Directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EElo1zenAYKl",
        "outputId": "9041ccf4-452f-488b-cf1d-7ef017b5c1e2"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d_w-Tj6O_0D4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import seaborn as sns\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "directory = \"/content/drive/MyDrive/Face Mask Detection/archive/Medical mask/Medical mask/Medical Mask/annotations\"\n",
        "image_directory = \"/content/drive/MyDrive/Face Mask Detection/archive/Medical mask/Medical mask/Medical Mask/images\"\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Face Mask Detection/archive/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/Face Mask Detection/archive/submission.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLtcb3ca_0D7"
      },
      "source": [
        "<a id=\"section-two\"></a>\n",
        "## What is SSD?\n",
        "SSD is **Single Shot Multibox Detector**. It is a technique that is used to detect objects in images using a single deep neural network. Basically its used for object detection in an image. By using a **base architecture of VGG-16 Architecture**, SSD is able to out perform other object detectors like YOLO and Faster R-CNN in both speed and accuracy. The architecture of SSD is given in the figure below. Training a SSD model from scratch will require a lot of data, so here I have imported pretrained weights **(Caffe Face Detector Model)** using OpenCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmIEXLAo_0D8"
      },
      "source": [
        "![SSD Architecture](https://www.researchgate.net/profile/Adam_Nowosielski/publication/332948824/figure/fig5/AS:767146284036100@1559913335810/The-model-of-Single-Shot-MultiBox-Detector-SSD-25.ppm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0xpuCo4U_0D8"
      },
      "source": [
        "cvNet = cv2.dnn.readNetFromCaffe('/content/drive/MyDrive/Face Mask Detection/caffe-face-detector-opencv-pretrained-model/architecture.txt','/content/drive/MyDrive/Face Mask Detection/caffe-face-detector-opencv-pretrained-model/weights.caffemodel')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwF0Bd8n_0D9"
      },
      "source": [
        "<a id=\"section-three\"></a>\n",
        "## Functions \n",
        "1. JSON Function fetches the json file that has the data of bounding box in the training dataset.\n",
        "2. Gamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems. In simple terms it is used to instill some light in the image. If gamma < 1, image will shift towards darker end of the spectrum and when gamma > 1, there will be more light in the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "RqTUSDsO_0D9"
      },
      "source": [
        "def getJSON(filePathandName):\n",
        "    with open(filePathandName,'r') as f:\n",
        "        return json.load(f)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "U7k8dJ_J_0D-"
      },
      "source": [
        "def adjust_gamma(image, gamma=1.0):\n",
        "    invGamma = 1.0 / gamma\n",
        "    table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n",
        "    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZgx0wUe_0D-"
      },
      "source": [
        "<a id=\"section-four\"></a>\n",
        "## Data Preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOMNsKE9_0D_"
      },
      "source": [
        "Lets look at the JSON data given for training:\n",
        "* The Annotations field contains the data of all the faces present in a particular image.\n",
        "* There are various classnames but the true classnames are **face_with_mask** and **face_no_mask**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIFNTDNU_0EA",
        "outputId": "7145c7b6-52f8-4c3d-8b08-8772207dd7e5"
      },
      "source": [
        "jsonfiles= []\n",
        "for i in os.listdir(directory):\n",
        "    jsonfiles.append(getJSON(os.path.join(directory,i)))\n",
        "jsonfiles[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Annotations': [{'Attributes': {},\n",
              "   'BoundingBox': [96, 0, 430, 432],\n",
              "   'Confidence': 1,\n",
              "   'ID': 964820035886422400,\n",
              "   'classname': 'face_with_mask',\n",
              "   'isProtected': False},\n",
              "  {'Attributes': {},\n",
              "   'BoundingBox': [119, 219, 411, 430],\n",
              "   'Confidence': 1,\n",
              "   'ID': 571368820964801024,\n",
              "   'classname': 'mask_surgical',\n",
              "   'isProtected': False},\n",
              "  {'Attributes': {},\n",
              "   'BoundingBox': [77, 49, 480, 500],\n",
              "   'Confidence': 1,\n",
              "   'ID': 131405119765588880,\n",
              "   'classname': 'face_shield',\n",
              "   'isProtected': False}],\n",
              " 'FileName': '5317.jpg',\n",
              " 'NumOfAnno': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "tyrh5NhV_0EA",
        "outputId": "a9d26f7f-d0d9-4a11-b498-b4342eaa475a"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Face Mask Detection/archive/train.csv\")\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>y1</th>\n",
              "      <th>y2</th>\n",
              "      <th>classname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2756.png</td>\n",
              "      <td>69</td>\n",
              "      <td>126</td>\n",
              "      <td>294</td>\n",
              "      <td>392</td>\n",
              "      <td>face_with_mask</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2756.png</td>\n",
              "      <td>505</td>\n",
              "      <td>10</td>\n",
              "      <td>723</td>\n",
              "      <td>283</td>\n",
              "      <td>face_with_mask</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2756.png</td>\n",
              "      <td>75</td>\n",
              "      <td>252</td>\n",
              "      <td>264</td>\n",
              "      <td>390</td>\n",
              "      <td>mask_colorful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2756.png</td>\n",
              "      <td>521</td>\n",
              "      <td>136</td>\n",
              "      <td>711</td>\n",
              "      <td>277</td>\n",
              "      <td>mask_colorful</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6098.jpg</td>\n",
              "      <td>360</td>\n",
              "      <td>85</td>\n",
              "      <td>728</td>\n",
              "      <td>653</td>\n",
              "      <td>face_no_mask</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       name   x1   x2   y1   y2       classname\n",
              "0  2756.png   69  126  294  392  face_with_mask\n",
              "1  2756.png  505   10  723  283  face_with_mask\n",
              "2  2756.png   75  252  264  390   mask_colorful\n",
              "3  2756.png  521  136  711  277   mask_colorful\n",
              "4  6098.jpg  360   85  728  653    face_no_mask"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy4Vh--d_0EB"
      },
      "source": [
        "* By using the mask label and non_mask label, the bounding box data from json files is extracted.\n",
        "* The faces from any particular image are extracted and stored in the data list along with its label for the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "I73sEfsr_0EC"
      },
      "source": [
        "data = []\n",
        "img_size = 124\n",
        "mask = ['face_with_mask']\n",
        "non_mask = [\"face_no_mask\"]\n",
        "labels={'mask':0,'without mask':1}\n",
        "for i in df[\"name\"].unique():\n",
        "    f = i+\".json\"\n",
        "    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n",
        "        if j[\"classname\"] in mask:\n",
        "            x,y,w,h = j[\"BoundingBox\"]\n",
        "            img = cv2.imread(os.path.join(image_directory,i),1)\n",
        "            img = img[y:h,x:w]\n",
        "            img = cv2.resize(img,(img_size,img_size))\n",
        "            data.append([img,labels[\"mask\"]])\n",
        "        if j[\"classname\"] in non_mask:\n",
        "            x,y,w,h = j[\"BoundingBox\"]\n",
        "            img = cv2.imread(os.path.join(image_directory,i),1)\n",
        "            img = img[y:h,x:w]\n",
        "            img = cv2.resize(img,(img_size,img_size))    \n",
        "            data.append([img,labels[\"without mask\"]])\n",
        "random.shuffle(data)    "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4jsm0p9_0EC",
        "outputId": "04b28622-5fef-4c65-954f-6a4c074f529b"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5749"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQqGJnE8_0ED"
      },
      "source": [
        "* The visualization below tells us that the **Number of Mask images > Number of Non-Mask images**, so this is an imbalanced dataset. But since we are using a SSD pretrained model, which is trained to detect non-mask faces, this imbalance would not matter a lot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "6f8S4-7K_0ED",
        "outputId": "6f91dcce-f772-44a5-f419-8c9244ebdc8c"
      },
      "source": [
        "p = []\n",
        "for face in data:\n",
        "    if(face[1] == 0):\n",
        "        p.append(\"Mask\")\n",
        "    else:\n",
        "        p.append(\"No Mask\")\n",
        "sns.countplot(p)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7eff51253dd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eyZV-BDU_0EE"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "for features,label in data:\n",
        "    X.append(features)\n",
        "    Y.append(label)\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSxxvkPl_0EE",
        "outputId": "e81c75ff-54a9-4577-f214-f3e84b3b37cc"
      },
      "source": [
        "X[0].shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(124, 124, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "B4tTOqp-_0EF"
      },
      "source": [
        "X = np.array(X)/255.0\n",
        "X = X.reshape(-1,124,124,3)\n",
        "Y = np.array(Y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULlinskG_0EF",
        "outputId": "270704c9-94d5-48f4-c2ec-fae2bd3e0fec"
      },
      "source": [
        "np.unique(Y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_yqUgFS_0EG",
        "outputId": "257e5864-8a58-4146-acf1-9a92d2962121"
      },
      "source": [
        "Y.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5749,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNw6SXqG_0EH"
      },
      "source": [
        "<a id=\"section-five\"></a>\n",
        "## Model Architecture and Training Process "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "11fxuBUn_0EH"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbUIMDHt_0EI",
        "outputId": "c50a4d99-8433-4613-fc2c-03787cb05f89"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 124, 124, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 62, 62, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 60, 60, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                1254450   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 1,347,749\n",
            "Trainable params: 1,347,749\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "N7ih857t_0EI"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wCwDoydF_0EJ"
      },
      "source": [
        "xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "v7Trno_w_0EJ"
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  \n",
        "        samplewise_center=False,  \n",
        "        featurewise_std_normalization=False,  \n",
        "        samplewise_std_normalization=False,  \n",
        "        zca_whitening=False,    \n",
        "        rotation_range=15,    \n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,  \n",
        "        horizontal_flip=True,  \n",
        "        vertical_flip=False)\n",
        "datagen.fit(xtrain)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFP-sU8L_0EJ",
        "outputId": "e813d2fc-6520-49c1-90bb-690daf26df06"
      },
      "source": [
        "history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n",
        "                    steps_per_epoch=xtrain.shape[0]//32,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=(xval, yval))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "143/143 [==============================] - 48s 114ms/step - loss: 0.4221 - accuracy: 0.8033 - val_loss: 0.2354 - val_accuracy: 0.9174\n",
            "Epoch 2/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2641 - accuracy: 0.8906 - val_loss: 0.2619 - val_accuracy: 0.8913\n",
            "Epoch 3/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.2747 - accuracy: 0.8918 - val_loss: 0.2118 - val_accuracy: 0.9209\n",
            "Epoch 4/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.2479 - accuracy: 0.8973 - val_loss: 0.2061 - val_accuracy: 0.9130\n",
            "Epoch 5/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2445 - accuracy: 0.8983 - val_loss: 0.2194 - val_accuracy: 0.9174\n",
            "Epoch 6/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2327 - accuracy: 0.9037 - val_loss: 0.2204 - val_accuracy: 0.9096\n",
            "Epoch 7/100\n",
            "143/143 [==============================] - 15s 106ms/step - loss: 0.2164 - accuracy: 0.9088 - val_loss: 0.1749 - val_accuracy: 0.9330\n",
            "Epoch 8/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.2116 - accuracy: 0.9147 - val_loss: 0.1771 - val_accuracy: 0.9270\n",
            "Epoch 9/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.2186 - accuracy: 0.9120 - val_loss: 0.1880 - val_accuracy: 0.9243\n",
            "Epoch 10/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2159 - accuracy: 0.9168 - val_loss: 0.1920 - val_accuracy: 0.9339\n",
            "Epoch 11/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2063 - accuracy: 0.9281 - val_loss: 0.1750 - val_accuracy: 0.9261\n",
            "Epoch 12/100\n",
            "143/143 [==============================] - 15s 106ms/step - loss: 0.2106 - accuracy: 0.9168 - val_loss: 0.2047 - val_accuracy: 0.9252\n",
            "Epoch 13/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1959 - accuracy: 0.9191 - val_loss: 0.1750 - val_accuracy: 0.9278\n",
            "Epoch 14/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1960 - accuracy: 0.9259 - val_loss: 0.1566 - val_accuracy: 0.9409\n",
            "Epoch 15/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.2004 - accuracy: 0.9198 - val_loss: 0.1535 - val_accuracy: 0.9409\n",
            "Epoch 16/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1783 - accuracy: 0.9300 - val_loss: 0.1430 - val_accuracy: 0.9391\n",
            "Epoch 17/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1829 - accuracy: 0.9328 - val_loss: 0.1472 - val_accuracy: 0.9452\n",
            "Epoch 18/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1656 - accuracy: 0.9400 - val_loss: 0.2229 - val_accuracy: 0.9174\n",
            "Epoch 19/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1992 - accuracy: 0.9259 - val_loss: 0.1452 - val_accuracy: 0.9417\n",
            "Epoch 20/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1671 - accuracy: 0.9427 - val_loss: 0.1680 - val_accuracy: 0.9357\n",
            "Epoch 21/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1760 - accuracy: 0.9322 - val_loss: 0.1538 - val_accuracy: 0.9435\n",
            "Epoch 22/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1770 - accuracy: 0.9400 - val_loss: 0.1414 - val_accuracy: 0.9461\n",
            "Epoch 23/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1563 - accuracy: 0.9427 - val_loss: 0.1674 - val_accuracy: 0.9391\n",
            "Epoch 24/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1569 - accuracy: 0.9448 - val_loss: 0.1519 - val_accuracy: 0.9452\n",
            "Epoch 25/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1466 - accuracy: 0.9454 - val_loss: 0.2103 - val_accuracy: 0.9261\n",
            "Epoch 26/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1613 - accuracy: 0.9355 - val_loss: 0.1405 - val_accuracy: 0.9496\n",
            "Epoch 27/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1478 - accuracy: 0.9472 - val_loss: 0.1223 - val_accuracy: 0.9591\n",
            "Epoch 28/100\n",
            "143/143 [==============================] - 15s 106ms/step - loss: 0.1375 - accuracy: 0.9520 - val_loss: 0.1195 - val_accuracy: 0.9539\n",
            "Epoch 29/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1296 - accuracy: 0.9555 - val_loss: 0.1313 - val_accuracy: 0.9513\n",
            "Epoch 30/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1388 - accuracy: 0.9457 - val_loss: 0.1484 - val_accuracy: 0.9461\n",
            "Epoch 31/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1358 - accuracy: 0.9512 - val_loss: 0.1317 - val_accuracy: 0.9513\n",
            "Epoch 32/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1449 - accuracy: 0.9485 - val_loss: 0.1240 - val_accuracy: 0.9504\n",
            "Epoch 33/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1279 - accuracy: 0.9545 - val_loss: 0.1190 - val_accuracy: 0.9565\n",
            "Epoch 34/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1317 - accuracy: 0.9499 - val_loss: 0.1206 - val_accuracy: 0.9539\n",
            "Epoch 35/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1193 - accuracy: 0.9599 - val_loss: 0.1047 - val_accuracy: 0.9643\n",
            "Epoch 36/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1186 - accuracy: 0.9565 - val_loss: 0.1271 - val_accuracy: 0.9539\n",
            "Epoch 37/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1241 - accuracy: 0.9572 - val_loss: 0.1604 - val_accuracy: 0.9435\n",
            "Epoch 38/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1352 - accuracy: 0.9555 - val_loss: 0.1115 - val_accuracy: 0.9557\n",
            "Epoch 39/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1046 - accuracy: 0.9637 - val_loss: 0.1255 - val_accuracy: 0.9539\n",
            "Epoch 40/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1341 - accuracy: 0.9504 - val_loss: 0.1145 - val_accuracy: 0.9565\n",
            "Epoch 41/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1077 - accuracy: 0.9628 - val_loss: 0.1077 - val_accuracy: 0.9626\n",
            "Epoch 42/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1222 - accuracy: 0.9557 - val_loss: 0.1218 - val_accuracy: 0.9557\n",
            "Epoch 43/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1266 - accuracy: 0.9536 - val_loss: 0.1169 - val_accuracy: 0.9591\n",
            "Epoch 44/100\n",
            "143/143 [==============================] - 15s 107ms/step - loss: 0.1132 - accuracy: 0.9594 - val_loss: 0.1084 - val_accuracy: 0.9635\n",
            "Epoch 45/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1083 - accuracy: 0.9583 - val_loss: 0.1284 - val_accuracy: 0.9478\n",
            "Epoch 46/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1311 - accuracy: 0.9541 - val_loss: 0.1212 - val_accuracy: 0.9609\n",
            "Epoch 47/100\n",
            "143/143 [==============================] - 16s 110ms/step - loss: 0.1028 - accuracy: 0.9641 - val_loss: 0.1435 - val_accuracy: 0.9496\n",
            "Epoch 48/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1026 - accuracy: 0.9633 - val_loss: 0.1222 - val_accuracy: 0.9617\n",
            "Epoch 49/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1183 - accuracy: 0.9572 - val_loss: 0.1158 - val_accuracy: 0.9617\n",
            "Epoch 50/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1026 - accuracy: 0.9603 - val_loss: 0.1023 - val_accuracy: 0.9643\n",
            "Epoch 51/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.0981 - accuracy: 0.9634 - val_loss: 0.1535 - val_accuracy: 0.9513\n",
            "Epoch 52/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1140 - accuracy: 0.9565 - val_loss: 0.1004 - val_accuracy: 0.9661\n",
            "Epoch 53/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1104 - accuracy: 0.9579 - val_loss: 0.1048 - val_accuracy: 0.9635\n",
            "Epoch 54/100\n",
            "143/143 [==============================] - 16s 108ms/step - loss: 0.1018 - accuracy: 0.9596 - val_loss: 0.1042 - val_accuracy: 0.9591\n",
            "Epoch 55/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1027 - accuracy: 0.9629 - val_loss: 0.1205 - val_accuracy: 0.9522\n",
            "Epoch 56/100\n",
            "143/143 [==============================] - 16s 111ms/step - loss: 0.1105 - accuracy: 0.9642 - val_loss: 0.1326 - val_accuracy: 0.9574\n",
            "Epoch 57/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1153 - accuracy: 0.9572 - val_loss: 0.1227 - val_accuracy: 0.9617\n",
            "Epoch 58/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1032 - accuracy: 0.9636 - val_loss: 0.1100 - val_accuracy: 0.9617\n",
            "Epoch 59/100\n",
            "143/143 [==============================] - 15s 108ms/step - loss: 0.1025 - accuracy: 0.9635 - val_loss: 0.1348 - val_accuracy: 0.9565\n",
            "Epoch 60/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1012 - accuracy: 0.9647 - val_loss: 0.1271 - val_accuracy: 0.9522\n",
            "Epoch 61/100\n",
            "143/143 [==============================] - 16s 111ms/step - loss: 0.1228 - accuracy: 0.9592 - val_loss: 0.1067 - val_accuracy: 0.9635\n",
            "Epoch 62/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.0988 - accuracy: 0.9644 - val_loss: 0.1130 - val_accuracy: 0.9609\n",
            "Epoch 63/100\n",
            "143/143 [==============================] - 16s 108ms/step - loss: 0.1023 - accuracy: 0.9568 - val_loss: 0.1188 - val_accuracy: 0.9617\n",
            "Epoch 64/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1023 - accuracy: 0.9597 - val_loss: 0.1017 - val_accuracy: 0.9696\n",
            "Epoch 65/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.1102 - accuracy: 0.9609 - val_loss: 0.1127 - val_accuracy: 0.9670\n",
            "Epoch 66/100\n",
            "143/143 [==============================] - 16s 109ms/step - loss: 0.0905 - accuracy: 0.9651 - val_loss: 0.1212 - val_accuracy: 0.9574\n",
            "Epoch 67/100\n",
            " 75/143 [==============>...............] - ETA: 7s - loss: 0.0859 - accuracy: 0.9773"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz2dnJtM_0EK"
      },
      "source": [
        "<a id=\"section-six\"></a>\n",
        "## Training and Validation Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_VLGCHzi_0EK"
      },
      "source": [
        "plt.plot(history.history['accuracy'],'g')\n",
        "plt.plot(history.history['val_accuracy'],'b')\n",
        "plt.title('Training Accuracy vs Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3qjxVpKI_0EL"
      },
      "source": [
        "plt.plot(history.history['loss'],'g')\n",
        "plt.plot(history.history['val_loss'],'b')\n",
        "plt.title('Training Loss vs Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2wW4_H5_0EL"
      },
      "source": [
        "<a id=\"section-seven\"></a>\n",
        "## Model Testing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyopUImR_0EL"
      },
      "source": [
        "The test dataset has 1698 images and to evaluate the model I have taken a handful of images from this dataset as there are no labels for faces in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rcIsLqKR_0EM"
      },
      "source": [
        "print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mCiRicRt_0EM"
      },
      "source": [
        "test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YdkFlU__0EN"
      },
      "source": [
        "* Gamma Correction for making the image appear in more light.**(Gamma = 2)**\n",
        "* blobFromImage creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels. [Refer OpenCV documentation]\n",
        "* The blob is passed through the SSD network and detections are made with some confidence score.\n",
        "* Define a threshold confidence score, above which the detection will be considered as a candidate of being a face. (In this case **confidence threshold = 0.2**)\n",
        "* All the detections that qualify the confidence score are then passed to the architecture for classification into mask or non-mask image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "sNeTPR3J_0EN"
      },
      "source": [
        "gamma = 2.0\n",
        "fig = plt.figure(figsize = (14,14))\n",
        "rows = 3\n",
        "cols = 2\n",
        "axes = []\n",
        "assign = {'0':'Mask','1':\"No Mask\"}\n",
        "for j,im in enumerate(test_images):\n",
        "    image =  cv2.imread(os.path.join(image_directory,im),1)\n",
        "    image =  adjust_gamma(image, gamma=gamma)\n",
        "    (h, w) = image.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    cvNet.setInput(blob)\n",
        "    detections = cvNet.forward()\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        try:\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            frame = image[startY:endY, startX:endX]\n",
        "            confidence = detections[0, 0, i, 2]\n",
        "            if confidence > 0.2:\n",
        "                im = cv2.resize(frame,(img_size,img_size))\n",
        "                im = np.array(im)/255.0\n",
        "                im = im.reshape(1,124,124,3)\n",
        "                result = model.predict(im)\n",
        "                if result>0.5:\n",
        "                    label_Y = 1\n",
        "                else:\n",
        "                    label_Y = 0\n",
        "                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n",
        "        \n",
        "        except:pass\n",
        "    axes.append(fig.add_subplot(rows, cols, j+1))\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oms-BAxe_0EO"
      },
      "source": [
        "<a id=\"section-eight\"></a>\n",
        "## Conclusion\n",
        "By analyzing the results it can be observed that the whole system works well for faces that have spatial dominance i.e. in image at (1,1), (1,2) and (2,1) but fails in case of (2,2) where the faces are small and occupy less space in the overall image. **To get better results, different image preprocessing techniques can be used, or confidence threshold can be kept lower, or one can try different blob size.**"
      ]
    }
  ]
}